ğŸ›’ Customer Spend Prediction â€“ Next 30 Days
ğŸ“Œ Project Overview

This project aims to predict how much a customer is likely to spend in the next 30 days based on their past transaction behavior.
The problem is framed as a supervised regression task and the solution is deployed using a Streamlit web application.

Since real production data was not available, we generated realistic synthetic transaction data and built a complete ML pipeline, from data generation to deployment.

ğŸ¯ Problem Statement

Given a customerâ€™s historical transaction behavior:

How frequently they purchase

How recently they purchased

How much they spend

Their purchasing patterns

ğŸ‘‰ Predict the total amount they will spend in the next 30 days

ğŸ§  Machine Learning Framing

Type: Supervised Learning

Task: Regression

Input (X): Customer historical behavioral features

Output (y): Future 30-day spend

ğŸ“ Project Structure
Hackathon_project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ transactions.csv        # Synthetic transaction data
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ spend_model.pkl         # Trained regression model
â”‚   â””â”€â”€ scaler.pkl              # Feature scaler
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ (optional modular scripts)
â”‚
â”œâ”€â”€ app.py                      # Streamlit web app
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ *.ipynb                     # Notebooks (data, training, experiments)

ğŸ§ª Step 1: Synthetic Data Generation

Generated ~5000 customers

Each customer has multiple transactions

Simulated realistic:

Transaction dates

Purchase amounts

Quantities

Product counts

Promotion usage

Key columns:

transaction_id

customer_id

transaction_date

total_amount

quantity

num_products

promotion_used

This ensures privacy and allows full control over data quality.

ğŸ§¹ Step 2: Data Preprocessing

Converted date columns to proper datetime format

Removed invalid transactions (negative or zero values)

Sorted data by customer and time

Ensured clean, model-ready data

âœ‚ï¸ Step 3: Cutoff Strategy (No Data Leakage)

To avoid data leakage:

Used a time-based cutoff

Transactions before cutoff â†’ features

Transactions after cutoff (30 days) â†’ target

This simulates a real-world prediction scenario.

ğŸ§© Step 4: Feature Engineering

Created customer-level behavioral features:

RFM Features

recency_days

frequency

monetary

Behavioral Features

avg_order_value

max_order_value

std_order_value

total_quantity

avg_quantity

Diversity & Promotion

avg_products_per_order

promo_usage_ratio

Temporal

customer_lifetime_days

Each row in the final dataset represents one customer.

ğŸ¯ Step 5: Target Variable

Target:

future_spend_30d = total spend in the next 30 days


Customers with no purchases in the next 30 days were assigned a target value of 0.

âš–ï¸ Step 6: Model Training & Comparison

We experimented with multiple regression models:

Linear Regression

Ridge Regression

Random Forest Regressor

Gradient Boosting Regressor

Final Model Selected

âœ… Random Forest Regressor

Reasons:

Handles non-linear customer behavior

Robust to noise and outliers

Strong performance on tabular data

ğŸ“Š Evaluation Metrics

Models were evaluated using:

MAE (Mean Absolute Error) â€“ business friendly

RMSE (Root Mean Squared Error) â€“ penalizes large errors

RÂ² Score â€“ explained variance

ğŸ“¦ Step 7: Model Serialization

For deployment:

Trained model saved as spend_model.pkl

Feature scaler saved as scaler.pkl

These artifacts are loaded directly by the Streamlit app.

ğŸš€ Step 8: Deployment (Streamlit App)

A Streamlit web application was built where:

Users input customer behavior features

The app applies the same preprocessing & scaling

The trained model predicts next 30-day spend in real time

How to Run the App
streamlit run .\app.py

ğŸ–¥ï¸ Streamlit App Features

Interactive numeric inputs

Promotion usage slider

One-click prediction

Clear display of predicted spend

ğŸ§  Key Learnings

Importance of time-based data splitting

Feature engineering drives model performance

Proper serialization enables smooth deployment

Separation of training and inference is critical

ğŸ¤ One-Line Project Explanation (For Hackathon / Interview)

â€œWe built an end-to-end machine learning pipeline to predict customer spend in the next 30 days using historical behavior, trained a regression model, and deployed it as an interactive Streamlit application.â€

âœ… Future Improvements

Add customer segmentation

Use real production data

Experiment with XGBoost / LightGBM

Add API deployment (FastAPI)

ğŸ Conclusion

This project demonstrates a complete real-world ML workflow:
from data generation â†’ modeling â†’ evaluation â†’ deployment,
making it suitable for hackathons, interviews, and portfolio projects.
